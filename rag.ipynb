{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T08:48:05.452816Z",
     "start_time": "2024-08-10T08:48:05.443763Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -q langchain langchain-community langchain-core sentence_transformers langchain-openai python-dotenv beautifulsoup4 langchain-chroma langchain-groq langchain-ollama FlagEmbedding peft gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T08:48:05.458916Z",
     "start_time": "2024-08-10T08:48:05.452816Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://mer.vin/2024/02/ollama-embedding/ - original source code\n",
    "import sys\n",
    "print(f\"Python interpreter: {sys.executable}\") # getting python interpreter\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # change model and embedding #c1\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "from urllib.parse import urlparse, unquote\n",
    "from pathlib import Path  \n",
    "\n",
    "import shutil\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from langchain.docstore.document import Document \n",
    "\n",
    "from FlagEmbedding import FlagReranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_page_name(docs):\n",
    "    for item in docs: \n",
    "        full_url = item.metadata.get(\"source\")\n",
    "        parsed_url = urlparse(full_url)\n",
    "        page_name = unquote(parsed_url.path.split('/')[-1])\n",
    "\n",
    "        item.metadata[\"id\"] = page_name\n",
    "        print(item.metadata[\"id\"])\n",
    "        \n",
    "\n",
    "def scrape_jina_ai_2(url: str) -> Document:  \n",
    "    response = requests.get(\"https://r.jina.ai/\" + url) \n",
    "    content = response.text\n",
    "    \n",
    "    # remove urls\n",
    "    content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', content)\n",
    "    \n",
    "    return Document(page_content=content, metadata={\"source\": url}) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T08:48:09.032238Z",
     "start_time": "2024-08-10T08:48:05.460162Z"
    }
   },
   "outputs": [],
   "source": [
    "# embedding = OpenAIEmbeddings( model = \"text-embedding-3-small\") # using openAI embedding\n",
    "\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"           # using bge as embedding\n",
    "encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ") \n",
    "\n",
    "# from LLM for devs: https://github.com/trancethehuman/ai-workshop-code/blob/main/Web_scraping_for_LLM_in_2024.ipynb\n",
    "print(\"Initializing vector store...\")\n",
    "\n",
    "urls = [\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Main_Page\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Getting_Started\", \n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Personal_Tricorder\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/The_Player\",\n",
    "    \n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Stars\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Crafting\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Combat\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Weapons\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Planets\",\n",
    "    \"https://frackinuniverse.miraheze.org/wiki/Biomes\"\n",
    "]\n",
    "\n",
    "# loading urls \n",
    "# docs = [WebBaseLoader(url).load() for url in urls]\n",
    "# docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "docs_list = [scrape_jina_ai_2(url) for url in urls] # scrape using Jina AI's reader\n",
    "\n",
    "print(\"BIOMES PAGE >>> \")\n",
    "print(docs_list[-1].page_content+\"\\n\")\n",
    "\n",
    "# set page name in metadata\n",
    "extract_page_name(docs_list)\n",
    "\n",
    "# split document into chunks\n",
    "# TODO: experiment with chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(doc_splits)}\")\n",
    "print(f\"Sample chunk\\n{doc_splits[0]}\\n\")\n",
    "\n",
    "\n",
    "# Convert documents to Embeddings and store them\n",
    "print(\"\\n--- Creating vector store ---\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding = embedding,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\":6})  # initialize retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":6})  # initialize retriever\n",
    "\n",
    "\n",
    "# Add this function after your existing imports and before the main code\n",
    "def rerank_and_select(query, documents, top_k=3):\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "    \n",
    "    # Prepare passages for reranking\n",
    "    passages = [doc.page_content for doc in documents]\n",
    "    \n",
    "    # rerank_scores is a list of floats, e.g., [0.95, 0.87, 0.76, ...]\n",
    "    rerank_scores = reranker.compute_score([[query, p] for p in passages])\n",
    "    \n",
    "    # Sort documents based on scores | ranked_results gets a sorted list of tuples (if an iterator of sets were passed, then sorted() returns a sorted list of sets), where each tuple is (float, Document)\n",
    "    ranked_results = sorted(zip(rerank_scores, documents), key=lambda x: x[0], reverse=True)\n",
    "    # INSIDES --> result of zip is an iterator of tuples, where each tuple is (float, Document) | iterators are 'consumed' only ONCE\n",
    "    #             key : the tuples based on the rerank_score, the 1st part element of the tuple\n",
    "    #             reverse: we want the largest score to be placed first (descending)\n",
    "    \n",
    "    # Select top k results\n",
    "    top_results = ranked_results[:top_k]\n",
    "    \n",
    "    # Extract the original documents from the top results\n",
    "    selected_docs = [result[1] for result in top_results]\n",
    "    \n",
    "    return selected_docs\n",
    "\n",
    "def retrieve_and_format(query):\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    \n",
    "    reranked_docs = rerank_and_select(query,relevant_docs)\n",
    "    \n",
    "    print('\\n>>>> SOURCES <<<<< :')\n",
    "    print([doc.metadata.get(\"id\") for doc in reranked_docs])\n",
    "    \n",
    "    print_page_contents(reranked_docs)\n",
    "    \n",
    "    return \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "\n",
    "def print_page_contents(docs):\n",
    "    i=1\n",
    "    for doc in docs:\n",
    "        print(f\"======= Doc {i} =======\")\n",
    "        print(doc.page_content)\n",
    "        # print(doc.page_content[:100])\n",
    "        i+=1\n",
    "        \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# accessing API keys locally\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# accessing API keys thorugh Google Colab's secret keys\n",
    "# from google.colab import userdata\n",
    "# api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "model_local = ChatOpenAI(   \n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature = 0.1,\n",
    "    api_key=api_key\n",
    "    )\n",
    "\n",
    "# api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# model_local = ChatGroq(\n",
    "#         api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "#         model=\"llama3-8b-8192\", # this > llama 3.1 8b-instant (performance)\n",
    "#         # model=\"llama-3.1-8b-instant\",\n",
    "#         # model=\"llama-3.1-70b-versatile\",\n",
    "#         temperature=0,\n",
    "#         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T09:02:28.621647Z",
     "start_time": "2024-08-10T09:02:22.879249Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# question = \"List out all one-handed melee weapons\"\n",
    "question = \"what are the features of the personal tricorder?\"\n",
    "exit_keyword = \"exit\"\n",
    "\n",
    "                \n",
    "llama_template = \"\"\"\n",
    "                <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                You are a pedantic but knowledgeable, efficient and direct AI assistant for Frackin' Universe Website. Provide concise answers focusing on key information. Offer tactful suggestions to solve the user's question. \n",
    "                Answer the *question* based only on the following *context*. If the information is not in the *context*, say that you don't have the informmation.\n",
    "                Focus on the MAIN contents of the given webpage in the *context*, ignoring the periphery of the website i.e., navigation bars, headers, footers, social media etc. \n",
    "                It is **CRITICAL** that you thoroughly digest the given *context* to answer the user's *question*.\n",
    "                Context: {context}\n",
    "                <|eot_id|>\n",
    "                <|start_header_id|>user<|end_header_id|>\n",
    "                {question}\n",
    "                <|eot_id|>\n",
    "                <|start_header_id|>assistant<|end_header_id|>\n",
    "                \"\"\"\n",
    "# rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "rag_prompt = ChatPromptTemplate.from_template(llama_template)\n",
    "rag_chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_and_format), \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")\n",
    "result = rag_chain.invoke(question)\n",
    "\n",
    "print(\"********ANSWER********\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T09:02:32.830437Z",
     "start_time": "2024-08-10T09:02:30.882750Z"
    }
   },
   "outputs": [],
   "source": [
    "tricorder_2 = \"where can i find this tricorder if i happen to lose it?\"\n",
    "result_2 = rag_chain.invoke(tricorder_2)\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T09:10:00.542907Z",
     "start_time": "2024-08-10T09:09:58.588426Z"
    }
   },
   "outputs": [],
   "source": [
    "tricorder_3 = \"Can i craft a personal tricorder without having to use my pixels?\"\n",
    "result_3 = rag_chain.invoke(tricorder_3)\n",
    "print(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T09:13:10.719230Z",
     "start_time": "2024-08-10T09:13:07.159514Z"
    }
   },
   "outputs": [],
   "source": [
    "tricorder_4 = \"can you tell me a few things that i can craft (along with its  required crafting amterials) with the tricorder?\"\n",
    "result_4 = rag_chain.invoke(tricorder_4)\n",
    "print(result_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stars\n",
    "q5 = \"what kinds of planets are in gentle stars?\"\n",
    "r5 = rag_chain.invoke(q5)\n",
    "print(r5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planets\n",
    "q6 = \"what locations can i find gelatinous planets?\" # only takes sources from the Stars page (in Planets, it says Gentle Stars, Temperate Stars etc.)\n",
    "r6 = rag_chain.invoke(q6) \n",
    "print(r6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planets\n",
    "# q7 = \"what's the fauna threat for gelatinous planets?\"\n",
    "q7 = \"what's the highest tier for a normal volcaninc planet?\"\n",
    "r7 = rag_chain.invoke(q7)\n",
    "print(r7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biomes\n",
    "q8 = \"what's the reason to visit for Bog biomes?\"\n",
    "r8 = rag_chain.invoke(q8)\n",
    "print(r8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T08:48:18.636528Z",
     "start_time": "2024-08-10T08:48:15.018442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://bb95cb682c80dcb8c7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bb95cb682c80dcb8c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "gr_embedding = embedding\n",
    "gr_documents = doc_splits\n",
    "gr_llm = model_local\n",
    "\n",
    "def process_input(urls, question):\n",
    "    # model_local = ChatOllama(model=\"mistral\")\n",
    "    \n",
    "    # Convert string of URLs to list\n",
    "    urls_list = urls.split(\"\\n\")\n",
    "    # docs = [WebBaseLoader(url).load() for url in urls_list]\n",
    "    # docs_list = [item for sublist in docs for item in sublist]\n",
    "    docs_list = [scrape_jina_ai_2(url) for url in urls_list] # scrape using Jina AI's reader\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
    "    # text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=gr_documents,\n",
    "        collection_name=\"rag-chroma\",\n",
    "        embedding=gr_embedding,\n",
    "    )\n",
    "    \n",
    "    # retriever = vectorstore.as_retriever()\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":4})\n",
    "\n",
    "    llama_template = \"\"\"\n",
    "                <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                You are a pedantic but knowledgeable, efficient and direct AI assistant for Frackin' Universe Website. Provide concise answers focusing on key information. Offer tactful suggestions to solve the user's question. \n",
    "                Answer the *question* based only on the following *context*. If the information is not in the *context*, say that you don't have the informmation.\n",
    "                Focus on the MAIN contents of the given webpage in the *context*, ignoring the periphery of the website i.e., navigation bars, headers, footers, social media etc. \n",
    "                It is **CRITICAL** that you thoroughly digest the given *context* to answer the user's *question*.\n",
    "                Context: {context}\n",
    "                <|eot_id|>\n",
    "                <|start_header_id|>user<|end_header_id|>\n",
    "                {question}\n",
    "                <|eot_id|>\n",
    "                <|start_header_id|>assistant<|end_header_id|>\n",
    "                \"\"\"\n",
    "    \n",
    "    after_rag_prompt = ChatPromptTemplate.from_template(llama_template)\n",
    "    after_rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | after_rag_prompt\n",
    "        # | model_local\n",
    "        | gr_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return after_rag_chain.invoke(question)\n",
    "\n",
    "# Define Gradio interface\n",
    "iface = gr.Interface(fn=process_input,\n",
    "                     inputs=[gr.Textbox(label=\"Enter URLs separated by new lines\"), gr.Textbox(label=\"Question\")],\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Webpage Query\",\n",
    "                     description=\"Enter URLs and a question to query the documents.\")\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
